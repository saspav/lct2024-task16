{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8697510,"sourceType":"datasetVersion","datasetId":5082283}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T14:40:53.004478Z","iopub.execute_input":"2024-06-15T14:40:53.004953Z","iopub.status.idle":"2024-06-15T14:40:54.667342Z","shell.execute_reply.started":"2024-06-15T14:40:53.004919Z","shell.execute_reply":"2024-06-15T14:40:54.665845Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/x5-tech-ai-hack/train_data_-400.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_5.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_7.csv\n/kaggle/input/x5-tech-ai-hack/train_merged.csv\n/kaggle/input/x5-tech-ai-hack/train_chunks_512.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_3.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_7.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_7.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_5.csv\n/kaggle/input/x5-tech-ai-hack/test_extend.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_6.csv\n/kaggle/input/x5-tech-ai-hack/train_extend.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_3.csv\n/kaggle/input/x5-tech-ai-hack/train_merged_-400.csv\n/kaggle/input/x5-tech-ai-hack/train_Collection5.json\n/kaggle/input/x5-tech-ai-hack/test_extend_2.csv\n/kaggle/input/x5-tech-ai-hack/train.json\n/kaggle/input/x5-tech-ai-hack/train_extend_2.csv\n/kaggle/input/x5-tech-ai-hack/gt_test.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_3.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_5.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_4.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_2.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_8.csv\n/kaggle/input/x5-tech-ai-hack/train_data.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_8.csv\n/kaggle/input/x5-tech-ai-hack/train_test_extend_6.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_6.csv\n/kaggle/input/x5-tech-ai-hack/train_test_merged.csv\n/kaggle/input/x5-tech-ai-hack/test_extend_4.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_4.csv\n/kaggle/input/x5-tech-ai-hack/train_extend_8.csv\n/kaggle/input/x5-tech-ai-hack/model/config.json\n/kaggle/input/x5-tech-ai-hack/model/tokenizer_config.json\n/kaggle/input/x5-tech-ai-hack/model/.gitkeep\n/kaggle/input/x5-tech-ai-hack/model/model.safetensors\n/kaggle/input/x5-tech-ai-hack/model/special_tokens_map.json\n/kaggle/input/x5-tech-ai-hack/model/vocab.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom tqdm import tqdm\nfrom collections import Counter, defaultdict\nfrom transformers import BertTokenizer, BertForTokenClassification\nfrom transformers import DebertaV2Tokenizer, DebertaV2ForTokenClassification\n\n# пытаемся импортировать самодельный экспорт в эксель с красивостями\ntry:\n    from df_addons import df_to_excel\nexcept ModuleNotFoundError:\n    df_to_excel = lambda sdf, spt, *args, **kwargs: sdf.to_excel(spt, index=False)\n\n__import__('warnings').filterwarnings(\"ignore\")\n\nMAX_LEN = 512\nOVERLAP = 0.2\n\ntarget_labels = ['B-value', 'I-value', 'B-discount']\ntag_values = target_labels + ['O', 'PAD']\ntag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\nidx2tag = {idx: tag for tag, idx in tag2idx.items()}\n\n\nclass DataTransform:\n    \"\"\" Класс для поиска сущностей\"\"\"\n\n    def __init__(self, model_name=None, model_path=None,\n                 tokenizer=None, token_classification=None,\n                 load_model=True, cuda=True):\n        \"\"\"\n        Инициализация экземпляра класса\n        :param model_name: имя модели\n        :param model_path: путь к предобученной модели\n        :param tokenizer: токенизатор\n        :param token_classification: классификатор\n        :param cuda: использовать GPU\n        \"\"\"\n\n        # Используем GPU если доступно\n        self.device = torch.device('cuda' if torch.cuda.is_available() and cuda else 'cpu')\n\n        # путь к локальной модели\n        if model_name is None:\n            self.model_name = './model'\n        else:\n            self.model_name = model_name\n\n        # путь к локальной модели\n        if model_path is None:\n            self.model_path = './model'\n        else:\n            self.model_path = model_path\n\n        if load_model:\n            if tokenizer is None:\n                tokenizer = BertTokenizer\n\n            self.tokenizer = tokenizer.from_pretrained(self.model_name,\n                                                       do_lower_case=False)\n\n            if token_classification is None:\n                token_classification = BertForTokenClassification\n            # Загрузка модели\n            self.model = token_classification.from_pretrained(self.model_path,\n                                                              num_labels=len(tag2idx),\n                                                              output_attentions=False,\n                                                              output_hidden_states=False,\n                                                              ignore_mismatched_sizes=True,\n                                                              )\n            self.model.to(self.device)\n            self.model.eval()\n\n        else:\n            self.model = None\n\n        # Использовать регулярки для поиска сущностей\n        self.regexp_text = False\n        self.discount_pattern = re.compile('скид[ко]\\w*', flags=re.IGNORECASE)\n        self.percent_pattern = re.compile('процент\\w*', flags=re.IGNORECASE)\n\n    def preprocess_text(self, text):\n        \"\"\"\n        Преобразование текста в нужный формат: токенизация и преобразование входных данных\n        :param text: текст\n        :return: words, tokenized_words, input_ids, attention_mask\n        \"\"\"\n        words = text.lower().split()\n        tokenized_words = self.tokenizer.tokenize(text.lower())\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_words)\n\n        input_ids = input_ids[:MAX_LEN]\n\n        attention_mask = [1] * len(input_ids)\n\n        return words, tokenized_words, input_ids, attention_mask\n\n    def split_text_with_overlap(self, text, max_len=MAX_LEN, overlap=OVERLAP):\n        \"\"\"\n        Разделить текст на части с перекрытием.\n        :param text: текст для разделения\n        :param max_len: максимальная длина части\n        :param overlap: процент перекрытия\n        :return: список частей текста\n        \"\"\"\n\n        words = text.lower().split()\n        chunk_size = max(max_len - 12, 10)  # Оставляем место для специальных токенов\n        overlap_size = int(chunk_size * overlap)\n        chunks = []\n        start_index = end_index = 0\n        while end_index < len(words):\n            end_index = start_index + chunk_size\n            chunk_words = words[start_index:end_index]\n            chunk_text = ' '.join(chunk_words)\n\n            # Токенизируем текст, чтобы убедиться, что кол-во токенов не превышает max_len\n            tokens = self.tokenizer.tokenize(chunk_text)\n            while len(tokens) > chunk_size and len(chunk_words):\n                chunk_words = chunk_words[:-1]\n                chunk_text = ' '.join(chunk_words)\n                tokens = self.tokenizer.tokenize(chunk_text)\n\n            chunks.append((chunk_text, start_index))\n\n            # У нас один чанк и нечего дальше крутить цикл - виснет\n            if len(chunk_words) == len(words):\n                break\n\n            # найдем сколько слов входит в перекрытие для вычисления индекса смещения\n            tokens = []\n            overlap_index = 0\n            reversed_words = chunk_words[::-1]\n            # пока длина токенов перекрытия меньше размера перекрытия добавляем по слову\n            while len(tokens) < overlap_size:\n                overlap_index += 1\n                tokens = self.tokenizer.tokenize(' '.join(reversed_words[:overlap_index]))\n\n            end_index = start_index + len(chunk_words)\n            # Следующая часть начинается с учетом перекрытия\n            start_index += len(chunk_words) - overlap_index\n\n        return chunks\n\n    @staticmethod\n    def get_words_positions(input_words, pattern):\n        found_index = []\n        for idx, word in enumerate(input_words):\n            if pattern.match(word):\n                found_index.append(idx)\n        return found_index\n\n    def get_entities(self, text):\n        \"\"\"\n        Функция принимает на вход текст и возвращает найденные сущности и их индексы.\n        :param text: текст\n        :return: найденные сущности и их индексы\n        \"\"\"\n        # Разделить текст на части с перекрытием\n        chunks = self.split_text_with_overlap(text, MAX_LEN, OVERLAP)\n        all_labels_positions = []\n        all_results = []\n\n        # print(f'\\nlen(chunks): {len(chunks)}', 'chunks:', *chunks, sep='\\n')\n\n        for chunk, start_index in chunks:\n            words, tokenized_words, input_ids, attention_mask = self.preprocess_text(chunk)\n\n            # Создайте тензоры для входных данных\n            input_ids = torch.tensor(input_ids).unsqueeze(0).to(self.device)\n            attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(self.device)\n\n            # Прогоните текст через модель для предсказания меток\n            with torch.no_grad():\n                outputs = self.model(input_ids, attention_mask=attention_mask)\n\n            logits = outputs[0].detach().cpu().numpy()\n            predicted_labels = np.argmax(logits, axis=2)[0]\n\n            labels_positions, result = self.get_entities_with_labels(tokenized_words,\n                                                                     predicted_labels,\n                                                                     start_index)\n\n            # print('labels_positions:', labels_positions, result)\n\n            all_labels_positions.append(labels_positions)\n            all_results.extend(result)\n\n        words = text.lower().split()\n\n        # Если используем регулярки для дополнительного поиска сущностей\n        if self.regexp_text:\n            labels_positions, result = {}, []\n            found_idxs = self.get_words_positions(words, self.discount_pattern)\n            if found_idxs:\n                labels_positions['B-discount'] = found_idxs\n                result.extend([(words[idx], 2, idx) for idx in found_idxs])\n                # found_idxs = self.get_words_positions(words, self.percent_pattern)\n                # labels_positions['I-value'] = found_idxs\n                # result.extend([(words[idx], 1, idx) for idx in found_idxs])\n                all_labels_positions.append(labels_positions)\n                all_results.extend(result)\n\n        # print('all_lpr:', all_labels_positions, all_results, len(text.split()), sep='\\n')\n\n        try:\n            # Удаление дублированных меток и приведение к исходному тексту\n            labels_positions, final_results = self.merge_chunks_results(all_labels_positions,\n                                                                        all_results,\n                                                                        len(words))\n        except TypeError as err:\n            print(err)\n            print(text)\n            print(all_labels_positions, all_results, len(text.split()), sep='\\n')\n\n        # print('lp_fr:', labels_positions, final_results, len(text.split()), sep='\\n')\n\n        # если это список словарей\n        if isinstance(labels_positions, list):\n            # объединение позиций слов по ключам\n            combined_values = defaultdict(list)\n            for item in labels_positions:\n                for key, values in item.items():\n                    combined_values[key].extend(values)\n            labels_positions = {key: sorted(set(combined_values[key]))\n                                for key in sorted(combined_values)}\n\n        final_positions = []\n        for values in labels_positions.values():\n            final_positions.extend(values)\n        final_positions = [(idx, words[idx]) for idx in sorted(final_positions)]\n\n        return labels_positions, final_results, final_positions\n\n    def get_entities_with_labels(self, tokenized_words, predicted_labels, start_index):\n        \"\"\"\n        Объединение токенов в сущности с метками\n        :param tokenized_words: токенизированные слова\n        :param predicted_labels: предсказанные метки\n        :param start_index: начальный индекс чанка\n        :return:\n        \"\"\"\n        current_word = \"\"\n        current_label = []\n        words_with_labels = []\n\n        # print('token_words, pred_labels:', tokenized_words, predicted_labels, sep='\\n')\n\n        if isinstance(self.tokenizer, DebertaV2Tokenizer):\n            for token, label in zip(tokenized_words, predicted_labels):\n                if token.startswith('▁'):\n                    if current_word:\n                        words_with_labels.append((current_word, current_label))\n                    current_word = token[1:]\n                    current_label = [label]\n                else:\n                    current_word += token\n                    current_label.append(label)\n\n        else:\n            for token, label in zip(tokenized_words, predicted_labels):\n                if token.startswith(\"##\"):\n                    current_word += token[2:]\n                    current_label.append(label)\n                else:\n                    if current_word:\n                        words_with_labels.append((current_word, current_label))\n                    current_word = token\n                    current_label = [label]\n\n        if current_word:\n            words_with_labels.append((current_word, current_label))\n\n        result = []\n        previous_label = None\n        previous_index = None\n        for idx, (word, labels) in enumerate(words_with_labels):\n            label = Counter(labels).most_common(1)[0][0]\n            if label < 3:\n                if label != 2:\n                    label = int(previous_index is not None and\n                                previous_label is not None and\n                                idx == previous_index + 1 and previous_label < 2)\n                result.append((word, label, start_index + idx))\n                previous_label = label\n                previous_index = idx\n\n        target_labels_positions = {}\n        for target, target_label in enumerate(target_labels):\n            labels = [idx for word, label, idx in result if label == target]\n            if labels:\n                target_labels_positions[target_label] = labels\n        return target_labels_positions, result\n\n    @staticmethod\n    def merge_chunks_results(labels_positions, results, original_length):\n        \"\"\"\n        Объединение результатов из перекрывающихся частей в один результат.\n        :param labels_positions: Список позиций меток из частей\n        :param results: список результатов из частей\n        :param original_length: длина оригинального текста в словах\n        :return: объединенные метки и результаты\n        \"\"\"\n        final_labels_positions = [*filter(bool, labels_positions)]\n        final_results = [3] * original_length\n\n        # print('labels_positions:', labels_positions)\n        # print('results:', results)\n\n        for word, label, idx in results:\n            if final_results[idx] > 2:\n                final_results[idx] = label\n\n        return final_labels_positions, final_results\n\n    def predict_entities(self, test_df):\n        \"\"\"\n        Поиск сущностей в датафрейме\n        :param test_df: ДФ\n        :return:\n        \"\"\"\n        true_labels = []\n        pred_labels = []\n        predict_labels = []\n        predict_results = []\n        for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n            input_text = row['processed_text']\n\n            if 'target_labels_positions' in row:\n                true_labels.append(self.transform_text_labels(input_text,\n                                                              row['target_labels_positions']))\n\n            entities, result, positions = self.get_entities(input_text)\n            predict_labels.append(entities)\n            predict_results.append(positions)\n\n            pred_labels.append(self.transform_text_labels(input_text, entities))\n\n        test_df['predict_labels'] = predict_labels\n        test_df['predict_results'] = predict_results\n\n        return test_df, true_labels, pred_labels\n\n    @staticmethod\n    def transform_text_labels(text, labels):\n        \"\"\"\n        Формирование списка меток для каждого слова в тексте.\n        :param text: текст со словами, разделенными пробелами\n        :param labels: список со словарями меток и их позициями\n        :return: список меток для каждого слова в тексте\n        \"\"\"\n        # если метки - это словарь, засунем его в список\n        if isinstance(labels, dict):\n            labels = [labels]\n        len_words = len(text.split())\n        idx_labels = ['O'] * len_words\n        for label_dict in labels:\n            for key, values in label_dict.items():\n                for value in values:\n                    if value < len_words:\n                        idx_labels[value] = key\n        return idx_labels","metadata":{"execution":{"iopub.status.busy":"2024-06-15T14:40:54.674887Z","iopub.execute_input":"2024-06-15T14:40:54.675682Z","iopub.status.idle":"2024-06-15T14:41:02.853158Z","shell.execute_reply.started":"2024-06-15T14:40:54.675648Z","shell.execute_reply":"2024-06-15T14:41:02.851879Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(f'/kaggle/input/x5-tech-ai-hack/gt_test.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T14:41:02.855626Z","iopub.execute_input":"2024-06-15T14:41:02.856186Z","iopub.status.idle":"2024-06-15T14:41:02.932266Z","shell.execute_reply.started":"2024-06-15T14:41:02.856153Z","shell.execute_reply":"2024-06-15T14:41:02.930816Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"bert_name = '/kaggle/input/x5-tech-ai-hack/model'\nbert_tuned = '/kaggle/input/x5-tech-ai-hack/model'\n\ndps = DataTransform(model_name=bert_name, model_path=bert_tuned)\n\nresult = pd.DataFrame(columns=['processed_text', 'label'])\n\nfor i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n    input_text = row['processed_text']\n\n    entities, *_ = dps.get_entities(input_text)\n\n    labels = dps.transform_text_labels(input_text, entities)\n\n    result.loc[len(result)] = [input_text, labels]\n\nresult.to_csv('gt_test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T14:41:02.933672Z","iopub.execute_input":"2024-06-15T14:41:02.934242Z","iopub.status.idle":"2024-06-15T14:45:22.006811Z","shell.execute_reply.started":"2024-06-15T14:41:02.934205Z","shell.execute_reply":"2024-06-15T14:45:22.005629Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|██████████| 482/482 [04:12<00:00,  1.91it/s]\n","output_type":"stream"}]}]}