{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-15T06:03:58.748709Z",
     "iopub.status.busy": "2024-06-15T06:03:58.748381Z",
     "iopub.status.idle": "2024-06-15T06:03:59.793006Z",
     "shell.execute_reply": "2024-06-15T06:03:59.791883Z",
     "shell.execute_reply.started": "2024-06-15T06:03:58.748683Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:03:59.795495Z",
     "iopub.status.busy": "2024-06-15T06:03:59.795005Z",
     "iopub.status.idle": "2024-06-15T06:04:42.216201Z",
     "shell.execute_reply": "2024-06-15T06:04:42.215012Z",
     "shell.execute_reply.started": "2024-06-15T06:03:59.795460Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install seqeval\n",
    "# !pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:04:42.221929Z",
     "iopub.status.busy": "2024-06-15T06:04:42.221646Z",
     "iopub.status.idle": "2024-06-15T06:04:59.165263Z",
     "shell.execute_reply": "2024-06-15T06:04:59.164253Z",
     "shell.execute_reply.started": "2024-06-15T06:04:42.221899Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from IPython.display import clear_output\n",
    "from nltk import pos_tag\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = DeprecationWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", message = \"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message = \"numpy.ufunc size changed\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action = \"ignore\", category = pd.errors.PerformanceWarning)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForTokenClassification\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from seqeval.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    pass \n",
    "\n",
    "clear_output(wait = False)\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"./models/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(\"./val/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(\"./results/checkpoint-last\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device, n_gpu)\n",
    "\n",
    "bert_name = 'DeepPavlov/rubert-base-cased-conversational'\n",
    "\n",
    "MAX_LEN = 512\n",
    "OVERLAP = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:04:59.166989Z",
     "iopub.status.busy": "2024-06-15T06:04:59.166445Z",
     "iopub.status.idle": "2024-06-15T06:04:59.815017Z",
     "shell.execute_reply": "2024-06-15T06:04:59.813971Z",
     "shell.execute_reply.started": "2024-06-15T06:04:59.166961Z"
    }
   },
   "outputs": [],
   "source": [
    "train_version = 8\n",
    "\n",
    "# Чтение файла train_extend.csv в датафрейм\n",
    "# df = pd.read_csv(f'/kaggle/input/train_extend_{train_version}.csv', sep=',')\n",
    "df = pd.read_csv(f'/kaggle/input/train_test_extend_{train_version}.csv', sep=',')\n",
    "# Преобразование значения в поле 'target_labels_positions' в словарь с помощью модуля ast\n",
    "df['target_labels_positions'] = df['target_labels_positions'].map(ast.literal_eval)\n",
    "\n",
    "target_labels = ['B-value', 'I-value', 'B-discount']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:04:59.816980Z",
     "iopub.status.busy": "2024-06-15T06:04:59.816530Z",
     "iopub.status.idle": "2024-06-15T06:05:00.612176Z",
     "shell.execute_reply": "2024-06-15T06:05:00.611256Z",
     "shell.execute_reply.started": "2024-06-15T06:04:59.816946Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, labels, row_labels = [], [], []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    words = row['processed_text'].split()\n",
    "    label_positions = row['target_labels_positions']\n",
    "    word_labels = ['O'] * len(words)\n",
    "\n",
    "    for label, positions in label_positions.items():\n",
    "        for pos in positions:\n",
    "            if 0 <= pos < len(words):\n",
    "                word_labels[pos] = label\n",
    "\n",
    "    sentences.append(words)\n",
    "    labels.append(word_labels)\n",
    "    row_labels.append(row['row_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.614220Z",
     "iopub.status.busy": "2024-06-15T06:05:00.613566Z",
     "iopub.status.idle": "2024-06-15T06:05:00.619221Z",
     "shell.execute_reply": "2024-06-15T06:05:00.618321Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.614169Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.620880Z",
     "iopub.status.busy": "2024-06-15T06:05:00.620536Z",
     "iopub.status.idle": "2024-06-15T06:05:00.631575Z",
     "shell.execute_reply": "2024-06-15T06:05:00.630582Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.620856Z"
    }
   },
   "outputs": [],
   "source": [
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.633001Z",
     "iopub.status.busy": "2024-06-15T06:05:00.632730Z",
     "iopub.status.idle": "2024-06-15T06:05:00.643520Z",
     "shell.execute_reply": "2024-06-15T06:05:00.642648Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.632977Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sentences[3394])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.647776Z",
     "iopub.status.busy": "2024-06-15T06:05:00.647523Z",
     "iopub.status.idle": "2024-06-15T06:05:00.654470Z",
     "shell.execute_reply": "2024-06-15T06:05:00.653453Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.647755Z"
    }
   },
   "outputs": [],
   "source": [
    "print(labels[3394])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.655698Z",
     "iopub.status.busy": "2024-06-15T06:05:00.655460Z",
     "iopub.status.idle": "2024-06-15T06:05:00.667874Z",
     "shell.execute_reply": "2024-06-15T06:05:00.666942Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.655677Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_values = target_labels + ['O', 'PAD']\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tag_values, len(tag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:00.669321Z",
     "iopub.status.busy": "2024-06-15T06:05:00.669024Z",
     "iopub.status.idle": "2024-06-15T06:05:01.474086Z",
     "shell.execute_reply": "2024-06-15T06:05:01.473100Z",
     "shell.execute_reply.started": "2024-06-15T06:05:00.669298Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.hist([len(s) for s in sentences], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:01.475653Z",
     "iopub.status.busy": "2024-06-15T06:05:01.475380Z",
     "iopub.status.idle": "2024-06-15T06:05:03.163775Z",
     "shell.execute_reply": "2024-06-15T06:05:03.162710Z",
     "shell.execute_reply.started": "2024-06-15T06:05:01.475628Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name, do_lower_case = False)\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained(bert_name, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:03.165465Z",
     "iopub.status.busy": "2024-06-15T06:05:03.165093Z",
     "iopub.status.idle": "2024-06-15T06:05:03.171998Z",
     "shell.execute_reply": "2024-06-15T06:05:03.171069Z",
     "shell.execute_reply.started": "2024-06-15T06:05:03.165432Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:03.174016Z",
     "iopub.status.busy": "2024-06-15T06:05:03.173587Z",
     "iopub.status.idle": "2024-06-15T06:05:03.195452Z",
     "shell.execute_reply": "2024-06-15T06:05:03.194553Z",
     "shell.execute_reply.started": "2024-06-15T06:05:03.173983Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_sentence_with_overlap(s_words, s_labels, tokenizer,\n",
    "                                max_len=MAX_LEN, overlap=OVERLAP):\n",
    "    \"\"\"\n",
    "    Разделить предложение на части с перекрытием.\n",
    "    :param s_words: предложение -> список слов для разделения\n",
    "    :param s_labels: список меток слов\n",
    "    :param tokenizer: экземпляр токенизатора\n",
    "    :param max_len: максимальная длина части\n",
    "    :param overlap: процент перекрытия\n",
    "    :return: список частей предложения и список частей меток\n",
    "    \"\"\"\n",
    "    chunk_size = max_len - 12  # Оставляем место для специальных токенов\n",
    "    overlap_size = int(chunk_size * overlap)\n",
    "    chunks = []\n",
    "    start_index = end_index = 0\n",
    "    while end_index < len(s_words):\n",
    "        end_index = start_index + chunk_size\n",
    "        chunk_words = s_words[start_index:end_index]\n",
    "        chunk_text = ' '.join(chunk_words)\n",
    "\n",
    "        # Токенизируем текст, чтобы убедиться в том, что длина в токенах не превышает max_len\n",
    "        tokens = tokenizer.tokenize(chunk_text)\n",
    "        while len(tokens) > chunk_size and len(chunk_words):\n",
    "            chunk_words = chunk_words[:-1]\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            tokens = tokenizer.tokenize(chunk_text)\n",
    "\n",
    "        chunks.append((chunk_words,\n",
    "                       s_labels[start_index: start_index + len(chunk_words)],\n",
    "                       start_index))\n",
    "\n",
    "        # У нас один чанк и нечего дальше крутить цикл - виснет\n",
    "        if len(chunk_words) == len(s_words):\n",
    "            break\n",
    "\n",
    "        # найдем сколько слов входит в перекрытие для вычисления индекса смещения\n",
    "        tokens = []\n",
    "        overlap_index = 0\n",
    "        reversed_words = chunk_words[::-1]\n",
    "        # пока длина токенов перекрытия меньше размера перекрытия добавляем по одному слову\n",
    "        while len(tokens) < overlap_size:\n",
    "            overlap_index += 1\n",
    "            tokens = tokenizer.tokenize(' '.join(reversed_words[:overlap_index]))\n",
    "\n",
    "        end_index = start_index + len(chunk_words)\n",
    "        # Следующая часть начинается с учетом перекрытия\n",
    "        start_index += len(chunk_words) - overlap_index\n",
    "\n",
    "    new_words, new_labels = [], []\n",
    "    for chunk_words, chunk_labels, start_index in chunks:\n",
    "        new_words.append(chunk_words)\n",
    "        new_labels.append(chunk_labels)\n",
    "\n",
    "    return new_words, new_labels\n",
    "\n",
    "\n",
    "def split_sentences(sentences, labels, row_labels, tokenizer, max_len=MAX_LEN, overlap=OVERLAP):\n",
    "    \"\"\"\n",
    "    Обработка списка списков предложений и списка списков меток\n",
    "    :param sentences: список списков предложений\n",
    "    :param labels: список списков меток\n",
    "    :param row_labels: список меток предложений (нужно для стратификации)\n",
    "    :param tokenizer: экземпляр токенизатора\n",
    "    :param max_len: максимальная длина части\n",
    "    :param overlap: процент перекрытия\n",
    "    :return: новые списки предложений и меток\n",
    "    \"\"\"\n",
    "    new_sentences, new_labels, new_row_labels, idx_rows = [], [], [], []\n",
    "\n",
    "    print('Разделение предложений на части:')\n",
    "    for idx, (s_words, s_labels, s_row) in tqdm(enumerate(zip(sentences, labels, row_labels)),\n",
    "                                                total=len(sentences)):\n",
    "        spl_snt, spl_lbl = split_sentence_with_overlap(s_words,\n",
    "                                                       s_labels,\n",
    "                                                       tokenizer,\n",
    "                                                       max_len=max_len,\n",
    "                                                       overlap=overlap)\n",
    "        new_sentences.extend(spl_snt)\n",
    "        new_labels.extend(spl_lbl)\n",
    "        new_row_labels.extend([s_row] * len(spl_snt))\n",
    "        idx_rows.extend([idx] * len(spl_snt))\n",
    "\n",
    "    return new_sentences, new_labels, new_row_labels, idx_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:05:03.196983Z",
     "iopub.status.busy": "2024-06-15T06:05:03.196665Z",
     "iopub.status.idle": "2024-06-15T06:09:18.024206Z",
     "shell.execute_reply": "2024-06-15T06:09:18.023332Z",
     "shell.execute_reply.started": "2024-06-15T06:05:03.196949Z"
    }
   },
   "outputs": [],
   "source": [
    "# разделение длинных предложений на части, так чтобы одно предложение не выходило за предел MAX_LEN токенов\n",
    "\n",
    "print(f'Количество предложений до: {len(sentences)}\\n')\n",
    "\n",
    "sentences, labels, row_labels, idx_rows = split_sentences(sentences, labels, row_labels,\n",
    "                                                          tokenizer, overlap=0.25)\n",
    "\n",
    "print(f'\\nКоличество предложений после: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:09:18.025735Z",
     "iopub.status.busy": "2024-06-15T06:09:18.025448Z",
     "iopub.status.idle": "2024-06-15T06:10:41.946486Z",
     "shell.execute_reply": "2024-06-15T06:10:41.945650Z",
     "shell.execute_reply.started": "2024-06-15T06:09:18.025711Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:10:41.948005Z",
     "iopub.status.busy": "2024-06-15T06:10:41.947703Z",
     "iopub.status.idle": "2024-06-15T06:10:56.125965Z",
     "shell.execute_reply": "2024-06-15T06:10:56.125123Z",
     "shell.execute_reply.started": "2024-06-15T06:10:41.947980Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen = MAX_LEN, dtype = \"long\", value = 0.0,\n",
    "                          truncating = \"post\", padding = \"post\")\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen = MAX_LEN, value = tag2idx[\"PAD\"], padding = \"post\",\n",
    "                     dtype = \"long\", truncating = \"post\")\n",
    "\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:10:56.127630Z",
     "iopub.status.busy": "2024-06-15T06:10:56.127249Z",
     "iopub.status.idle": "2024-06-15T06:10:56.139736Z",
     "shell.execute_reply": "2024-06-15T06:10:56.138839Z",
     "shell.execute_reply.started": "2024-06-15T06:10:56.127596Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "    \n",
    "#     model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "        bert_name,\n",
    "        num_labels = len(tag2idx),\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=3e-5,\n",
    "        eps=1e-8\n",
    "    )    \n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:10:56.141851Z",
     "iopub.status.busy": "2024-06-15T06:10:56.141517Z",
     "iopub.status.idle": "2024-06-15T06:10:56.193489Z",
     "shell.execute_reply": "2024-06-15T06:10:56.192446Z",
     "shell.execute_reply.started": "2024-06-15T06:10:56.141821Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(name_model, best_score_total=0):\n",
    "    \n",
    "    epochs = 30\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = total_steps\n",
    "    )\n",
    "    ## Store the average loss after each epoch so we can plot them.\n",
    "    loss_values, validation_loss_values = [], []\n",
    "    best_score = 0\n",
    "    not_increase = 0\n",
    "    not_increase_stop = 5\n",
    "\n",
    "    #for _ in trange(epochs, desc = \"Epoch\"):\n",
    "    for epoch in range(epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            # Перенос батча на GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Убедитесь, что метки имеют тип LongTensor\n",
    "            b_labels = b_labels.long()\n",
    "\n",
    "            # Всегда очищайте любые ранее рассчитанные градиенты перед выполнением обратного прохода\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(\n",
    "                b_input_ids, \n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask, \n",
    "                labels=b_labels\n",
    "            )\n",
    "\n",
    "            # Получение потерь\n",
    "            loss = outputs[0]\n",
    "            # Выполнение обратного прохода для расчета градиентов\n",
    "            loss.backward()\n",
    "            # Отслеживание тренировочных потерь\n",
    "            total_loss += loss.item()\n",
    "            # Ограничение нормы градиента\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # Обновление параметров\n",
    "            optimizer.step()\n",
    "            # Обновление скорости обучения\n",
    "            scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        #print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "    \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        # Put the model into evaluation mode\n",
    "        model.eval()\n",
    "        # Reset the validation loss for this epoch.\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels = [], []\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Telling the model not to compute or store gradients,\n",
    "            # saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have not provided labels.\n",
    "                outputs = model(\n",
    "                    b_input_ids, \n",
    "                    token_type_ids = None,\n",
    "                    attention_mask = b_input_mask, \n",
    "                    labels = b_labels\n",
    "                )\n",
    "            # Move logits and labels to CPU\n",
    "            logits = outputs[1].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            eval_loss += outputs[0].mean().item()\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis = 2)])\n",
    "            true_labels.extend(label_ids)\n",
    "\n",
    "        eval_loss = eval_loss / len(valid_dataloader)\n",
    "        validation_loss_values.append(eval_loss)\n",
    "    \n",
    "        #print(\"Validation loss: {}\".format(eval_loss))\n",
    "        pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                     for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "        valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                      for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "        score_val = f1_score(\n",
    "                valid_tags, pred_tags, \n",
    "                labels = ['B-value', 'I-value', 'B-discount', 'O'], \n",
    "                average = 'macro'\n",
    "        )\n",
    "        #print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "        #print(\"Validation F1-Score: {}\".format(score_val))\n",
    "        if best_score < score_val:\n",
    "            best_score = score_val         \n",
    "            not_increase = 0         \n",
    "            if best_score_total < best_score:\n",
    "                best_score_total = best_score\n",
    "                msg = f'save model, {best_score_total}, {score_val}'\n",
    "                print(epoch, msg)\n",
    "                \n",
    "                # Save the fine-tuned model locally\n",
    "                model.save_pretrained(f\"./results/checkpoint-last\")\n",
    "                \n",
    "                try:\n",
    "                    Path(name_model).mkdir(parents = True, exist_ok = True)\n",
    "                    model.save_pretrained(name_model)\n",
    "                except:    \n",
    "                    pass\n",
    "            else:\n",
    "                msg = f'... {best_score_total}, {score_val}'\n",
    "                print(epoch, msg)\n",
    "        else:\n",
    "            not_increase += 1\n",
    "            msg = f'not_increase = {not_increase}, {best_score}, {score_val}'\n",
    "            print('epoch:', epoch, msg)\n",
    "            if not_increase >= not_increase_stop:                \n",
    "                print('not_increase = ', not_increase_stop)\n",
    "                break\n",
    "                \n",
    "    return best_score_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T06:10:56.195265Z",
     "iopub.status.busy": "2024-06-15T06:10:56.194881Z",
     "iopub.status.idle": "2024-06-15T06:10:57.128163Z",
     "shell.execute_reply": "2024-06-15T06:10:57.126953Z",
     "shell.execute_reply.started": "2024-06-15T06:10:56.195233Z"
    }
   },
   "outputs": [],
   "source": [
    "SEEDS = 1\n",
    "scores = []\n",
    "batch_size = 24\n",
    "\n",
    "best_score = 0 \n",
    "\n",
    "folds = 5\n",
    "\n",
    "for seed in (0, 13, 17, ):    \n",
    "    # kf = KFold(n_splits = 5, random_state = seed, shuffle = True)\n",
    "    kf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(input_ids, row_labels)):\n",
    "        \n",
    "        test_index = sorted(set([idx_rows[i] for i in val_index]))\n",
    "        df.iloc[test_index].to_csv(f'seed_{seed}_{train_version}_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        clear_output(wait = False)\n",
    "        print('SEED', seed, 'Fold', fold)\n",
    "        print(np.mean(scores) - np.std(scores))\n",
    "        \n",
    "        tr_inputs  = input_ids[train_index]\n",
    "        val_inputs = input_ids[val_index]\n",
    "        \n",
    "        tr_tags  = tags[train_index]\n",
    "        val_tags = tags[val_index]\n",
    "        \n",
    "        tr_masks  = np.array(attention_masks)[train_index]\n",
    "        val_masks = np.array(attention_masks)[val_index]\n",
    "        \n",
    "        tr_inputs = torch.tensor(tr_inputs)\n",
    "        val_inputs = torch.tensor(val_inputs)\n",
    "\n",
    "        tr_tags = torch.tensor(tr_tags, dtype=torch.long)\n",
    "        val_tags = torch.tensor(val_tags, dtype=torch.long)\n",
    "\n",
    "        tr_masks = torch.tensor(tr_masks)\n",
    "        val_masks = torch.tensor(val_masks)\n",
    "        \n",
    "        train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "        \n",
    "        valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler = valid_sampler, batch_size = batch_size)\n",
    "        \n",
    "        model, optimizer = get_model()\n",
    "        \n",
    "        name_model = f'./best_model-{fold}-{folds}'\n",
    "        score = train_model(name_model, best_score)\n",
    "        scores.append(score)   \n",
    "        best_score = score\n",
    "               \n",
    "        del model, optimizer; gc.collect()     \n",
    "        \n",
    "        # хватит одного фолда\n",
    "#         break\n",
    "        \n",
    "print(np.mean(scores) - np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T06:10:57.129142Z",
     "iopub.status.idle": "2024-06-15T06:10:57.129615Z",
     "shell.execute_reply": "2024-06-15T06:10:57.129399Z",
     "shell.execute_reply.started": "2024-06-15T06:10:57.129380Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, FileLink\n",
    "from zipfile import ZipFile, ZIP_DEFLATED as ZD\n",
    "from glob import glob\n",
    "\n",
    "files = glob(f'results/checkpoint-last/*.*') + glob('*.csv')\n",
    "zip_filename = f'model_DeepPavlov_{train_version}.zip'\n",
    "with ZipFile(zip_filename, 'w',  compression=ZD, compresslevel=9) as zip_file:\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        zip_file.write(filename)\n",
    "FileLink(zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5082283,
     "sourceId": 8697510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
